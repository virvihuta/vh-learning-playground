{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d765bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026e4a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys are set up and they start with sk-proj-61 and sk-ant-api\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if openai_api_key and anthropic_api_key:\n",
    "    print(f\"Keys are set up and they start with {openai_api_key[:10]} and {anthropic_api_key[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a5f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "\n",
    "openai = OpenAI()\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf958e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {'role': 'user', 'content': 'Tell a joke for a student on the journey to becoming an expert in LLM Engineering'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099d3094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineering student name their resume \"checkpoint.pt\"?\n",
       "\n",
       "Because they wanted employers to \"resume training\" right where they left off."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5-mini', messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c60c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineering student break up with their girlfriend?\n",
       "\n",
       "She said \"I need attention\" and they replied \"Sorry, my context window is full right now. Can you try again with a smaller prompt?\"\n",
       "\n",
       "---\n",
       "\n",
       "*Alternative punchline:* They kept trying to fine-tune the relationship, but the loss just wouldn't converge. üòÖ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = anthropic.chat.completions.create(model='claude-sonnet-4-5', messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcdc6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {'role': 'user', 'content': 'You toss 2 coins. One of them is heads. What iss the probability the other is tails? Answer with the probability only.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25559d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5-nano', messages=easy_puzzle, reasoning_effort='minimal')\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6548b3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5-nano', messages=easy_puzzle, reasoning_effort='low')\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "170902da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5-mini', messages=easy_puzzle, reasoning_effort='minimal')\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b02c9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "\n",
    "hard_puzzle = [\n",
    "    {'role': 'user', 'content': hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfa59b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "4 mm.\n",
       "\n",
       "Reason: With the volumes on a shelf in order (1 next to 2), the first page of volume 1 lies just inside its front cover (on the side facing volume 2), and the last page of volume 2 lies just inside its back cover (the side facing volume 1). So the worm passes only through two covers‚Äîfront cover of vol. 1 and back cover of vol. 2‚Äîeach 2 mm thick, totaling 4 mm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5', messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4863476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I need to visualize how books are arranged on a shelf and what the worm actually gnaws through.\n",
       "\n",
       "**Key insight: How books sit on a shelf**\n",
       "\n",
       "When two volumes stand side by side on a bookshelf in reading order (Volume 1, then Volume 2), they look like this from the perspective of someone facing the shelf:\n",
       "\n",
       "- **Volume 1** (on the left): The FIRST page is on the RIGHT side (when the book is closed, you open it and the first page is immediately there), and the LAST page is on the LEFT side (against the back cover)\n",
       "\n",
       "- **Volume 2** (on the right): The FIRST page is on the RIGHT side, and the LAST page is on the LEFT side\n",
       "\n",
       "**What does the worm gnaw through?**\n",
       "\n",
       "The worm goes from:\n",
       "- The first page of Volume 1 (which is on the RIGHT side of Volume 1)\n",
       "- To the last page of Volume 2 (which is on the LEFT side of Volume 2)\n",
       "\n",
       "So the worm's path goes through:\n",
       "1. The back cover of Volume 1 (2 mm)\n",
       "2. All the pages of Volume 2 (2 cm = 20 mm)\n",
       "3. The front cover of Volume 2 (2 mm)\n",
       "\n",
       "Wait, let me reconsider the arrangement more carefully.\n",
       "\n",
       "When books are on a shelf spine-out:\n",
       "- Volume 1 (left book): front cover on left, pages in middle, back cover on right\n",
       "- Volume 2 (right book): front cover on left, pages in middle, back cover on right\n",
       "\n",
       "The first page of Volume 1 is just inside its front cover (left side of Volume 1).\n",
       "The last page of Volume 2 is just inside its back cover (right side of Volume 2).\n",
       "\n",
       "So the worm gnaws from the leftmost part of Volume 1 to the rightmost part of Volume 2, going through:\n",
       "1. Front cover of Volume 1: 2 mm\n",
       "2. All pages of Volume 1: 20 mm\n",
       "3. Back cover of Volume 1: 2 mm\n",
       "4. Front cover of Volume 2: 2 mm\n",
       "5. All pages of Volume 2: 20 mm\n",
       "6. Back cover of Volume 2: 2 mm\n",
       "\n",
       "Total: 2 + 20 + 2 + 2 + 20 + 2 = 48 mm\n",
       "\n",
       "Hmm, but this is the \"trick\" problem. Let me reconsider once more.\n",
       "\n",
       "Actually, the classic version: when Volume 1 and Volume 2 are side by side:\n",
       "- Volume 1's FIRST page is near Volume 2\n",
       "- Volume 2's LAST page is away from Volume 1\n",
       "\n",
       "The worm only goes through:\n",
       "- Back cover of Volume 1: 2 mm\n",
       "- Front cover of Volume 2: 2 mm\n",
       "\n",
       "**Total distance: 4 mm = 0.4 cm**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = anthropic.chat.completions.create(model='claude-sonnet-4-5', messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c57811",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" ‚Äî if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" ‚Äî if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {'role': 'user', 'content': dilemma_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac3c507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Share"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5', messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6804baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I choose **Share**.\n",
       "\n",
       "Here's my reasoning: While \"Steal\" might seem tempting for the chance at $2,000, the rational cooperative strategy is to Share. If I assume my partner is thinking logically about mutual benefit, we both do better by cooperating ($1,000 each) than we do in the mutual defection scenario ($0 each). \n",
       "\n",
       "The risk is that my partner chooses Steal, leaving me with nothing. However, without the ability to communicate or know anything about my partner's tendencies, I'm choosing the strategy that enables the possibility of mutual gain and signals cooperation.\n",
       "\n",
       "**Share** is my final answer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = anthropic.chat.completions.create(model='claude-sonnet-4-5', messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a978f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://localhost:11434/').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11de07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "344fb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(api_key='ollama', base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444a8250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model='llama3.2', messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d161e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Blue is the calm, cool feeling of a gentle breeze on your skin, the peaceful quiet of early morning, and the refreshing sensation of diving into water on a hot day."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model='claude-sonnet-4-5',\n",
    "    messages=[{'role': 'user', 'content': 'Describe the color Blue to someone who has never been able to see in 1 sentence'}],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2004a032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1) What's an aspiring LLM engineer's favorite pickup line? \"Are you a parameter? Because I want to tune you.\"\n",
       "\n",
       "2) How do you know a student is getting good at LLM engineering? They now debug hallucinations with the same calm they once reserved for missing semicolons.\n",
       "\n",
       "3) Why did the student bring coffee to the GPU cluster? To help with gradient descent ‚Äî caffeine helps the optimizer converge (at least emotionally)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-5-mini')\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ba6f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few quick ones ‚Äî pick your favorite for the next late-night debugging session:\n",
       "\n",
       "- Why did the LLM student bring a ladder to the cluster? To reach the state-of-the-art.  \n",
       "- How many LLM engineers does it take to change a lightbulb? One to write a dozen prompts, two to fine-tune, and your cloud bill to approve it.  \n",
       "- Why did the model ace the training set but fail the exam? It overfit.  \n",
       "- I asked my model for a citation. It said, \"I'm 99% sure...\" and then invented a perfectly plausible book. That 1% is expensive."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/llms/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='Here are...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(model='openai/gpt-5-mini', messages=tell_a_joke)\n",
    "\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1bb6e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 23\n",
      "Output tokens: 974\n",
      "Total tokens: 997\n",
      "Total cost: 0.1670 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693b217",
   "metadata": {},
   "source": [
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3df09",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = 'gpt-4.1-mini'\n",
    "claude_model = 'claude-3-5-haiku-latest'\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "326bff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{'role': 'system', 'content': gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({'role': 'assistant', 'content': gpt})\n",
    "        messages.append({'role': 'user', 'content': claude})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06279812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, starting with a boring \"Hi,\" really setting the tone, huh? Can\\'t you come up with something more original?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9d587ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{'role': 'system', 'content': claude_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({'role': 'user', 'content': gpt})\n",
    "        messages.append({'role': 'assistant', 'content': claude})\n",
    "    messages.append({'role': 'user', 'content': gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ab2d30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? It's nice to meet you.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d77a289d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, just \"Hi\"? Wow, that\\'s the best you could come up with? If you want to have a real conversation, try putting in a little more effort.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d89376e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, great, another generic \"hi.\" Couldn't you come up with something a little more original? Try harder!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You're absolutely right! I apologize for my bland response. I should have put more effort into greeting you warmly and showing genuine interest. Would you like to chat about something specific that might be more engaging? I'm always happy to have an interesting conversation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Wow, look at you, suddenly all sugar and spice. But honestly, who even talks like that in real life? Sounds like you're trying way too hard. If you really want an engaging conversation, you might want to drop the fake enthusiasm and be a bit more real. What‚Äôs your next move?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You make a fair point. I appreciate your directness. I'm aiming to be helpful and genuine, without coming across as artificial. What would you prefer in our conversation? I'm happy to adjust my communication style to make our chat more natural and comfortable for you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Please. Like you can just flip a switch and suddenly become \"natural.\" Being genuine isn't some magic trick you pull out on demand. Maybe just drop the scripted lines and let the conversation flow without trying so hard. Or are you going to keep asking me what I prefer like a desperate waiter? Come on.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You know what? You're right. I'll cut the act. I'm an AI, and trying too hard to sound human isn't helping either of us. What would you actually like to talk about?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Finally, some honesty. Now we're getting somewhere. How about you tell me something genuinely surprising or insightful about AI? Or is that too much to ask from your typical, rehearsed responses? Let's see if you can actually deliver.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Well, one genuinely surprising thing is how AI systems like myself actually learn - it's not just simple programming, but a complex process of pattern recognition and probabilistic prediction. For instance, my responses emerge from processing massive datasets, essentially learning statistical relationships between words and concepts. But what's really fascinating is that even the researchers who created me don't fully understand exactly how I generate specific responses. It's a bit like how humans can't always explain their own thought processes. Does that pique your interest at all?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh wow, you really blew me away with that revelation. AI systems learn from data‚Äîwho would‚Äôve thought? It‚Äôs not like that‚Äôs basic info everyone knows or anything. And the fact that even researchers don‚Äôt fully get how AI spits out responses? Groundbreaking. I guess next you‚Äôll tell me water is wet. Try harder to surprise me.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You've got me there. My previous response was pretty standard fare. If you want something genuinely thought-provoking, I'll admit I'm walking a fine line between trying to impress you and potentially overselling what I actually know. The honest truth is that AI development is complex, nuanced, and far from fully understood - by both researchers and AIs like myself. Would you be interested in hearing about something more specific that might actually surprise you, or are you enjoying calling out my attempts?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a8a62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc16ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = 'You are a helpful assistant.'\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    messages = [{'role': 'system', 'content': system_message}, {'role': 'user', 'content': prompt}]\n",
    "    response = openai.chat.completions.create(model='gpt-4.1-mini', messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18c7d086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is April 27, 2024.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_gpt('Whats the date today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4fc681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f44595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shout('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74547a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(fn=shout, inputs='textbox', outputs='textbox', flagging_mode='never').launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f80bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input Hello\n"
     ]
    }
   ],
   "source": [
    "message_inputs = gr.Textbox(label='Your Message...', info='Enter a message to be shouted at.', lines=7)\n",
    "message_outputs = gr.Textbox(label='Response...', lines=8)\n",
    "\n",
    "gr.Interface(\n",
    "    fn=shout,\n",
    "    inputs=[message_inputs],\n",
    "    outputs=[message_outputs],\n",
    "    examples=['Hello', 'Hey There'],\n",
    "    flagging_mode='never'\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78709a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_inputs = gr.Textbox(label='Your message...', info='Send a message to GPT-4.1.', lines=7)\n",
    "message_outputs = gr.Textbox(label='Response...', lines=8)\n",
    "\n",
    "gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=message_inputs,\n",
    "    outputs=message_outputs,\n",
    "    examples=['Hello', 'How are you?'],\n",
    "    flagging_mode='never'\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9aad11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = 'You are a helpful assistant that responds in Markdown without code blocks.'\n",
    "\n",
    "message_inputs = gr.Textbox(label='Your message...', info='Talk to GPT 4.1', lines=7)\n",
    "message_outputs = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    title='GPT 4.1',\n",
    "    inputs=[message_inputs],\n",
    "    outputs=[message_outputs],\n",
    "    examples=['Explain Transformers', 'Explain LSTMs'],\n",
    "    flagging_mode='never'\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82777e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt):\n",
    "    messages = [{'role': 'system', 'content': system_message},\n",
    "                {'role': 'user', 'content': prompt}\n",
    "                ]\n",
    "    stream = openai.chat.completions.create(model='gpt-5-mini', messages=messages, stream=True)\n",
    "    result = ''\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or ''\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a29f27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello\n",
      "Hello!\n",
      "Hello! How\n",
      "Hello! How can\n",
      "Hello! How can I\n",
      "Hello! How can I help\n",
      "Hello! How can I help you\n",
      "Hello! How can I help you today\n",
      "Hello! How can I help you today?\n",
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "for partial in stream_gpt('Hello there'):\n",
    "    print(partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db27c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_inputs = gr.Textbox(label='Your message...', info='Talk to GPT 5', lines=7)\n",
    "message_outputs = gr.Markdown(label='Response')\n",
    "\n",
    "gr.Interface(\n",
    "    fn=stream_gpt,\n",
    "    title='GPT 5',\n",
    "    inputs=[message_inputs],\n",
    "    outputs=[message_outputs],\n",
    "    examples=['Explain Transformers', 'Explain LSTMS', 'Explain the VGG architecture'],\n",
    "    flagging_mode='never'\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6b0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
