{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d765bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026e4a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys are set up and they start with sk-proj-61 and sk-ant-api\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if openai_api_key and anthropic_api_key:\n",
    "    print(f\"Keys are set up and they start with {openai_api_key[:10]} and {anthropic_api_key[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a5f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "\n",
    "openai = OpenAI()\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf958e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {'role': 'user', 'content': 'Tell a joke for a student on the journey to becoming an expert in LLM Engineering'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099d3094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineering student name their resume \"checkpoint.pt\"?\n",
       "\n",
       "Because they wanted employers to \"resume training\" right where they left off."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5-mini', messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c60c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineering student break up with their girlfriend?\n",
       "\n",
       "She said \"I need attention\" and they replied \"Sorry, my context window is full right now. Can you try again with a smaller prompt?\"\n",
       "\n",
       "---\n",
       "\n",
       "*Alternative punchline:* They kept trying to fine-tune the relationship, but the loss just wouldn't converge. üòÖ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = anthropic.chat.completions.create(model='claude-sonnet-4-5', messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcdc6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {'role': 'user', 'content': 'You toss 2 coins. One of them is heads. What iss the probability the other is tails? Answer with the probability only.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25559d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5-nano', messages=easy_puzzle, reasoning_effort='minimal')\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6548b3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5-nano', messages=easy_puzzle, reasoning_effort='low')\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "170902da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5-mini', messages=easy_puzzle, reasoning_effort='minimal')\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b02c9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "\n",
    "hard_puzzle = [\n",
    "    {'role': 'user', 'content': hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfa59b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "4 mm.\n",
       "\n",
       "Reason: With the volumes on a shelf in order (1 next to 2), the first page of volume 1 lies just inside its front cover (on the side facing volume 2), and the last page of volume 2 lies just inside its back cover (the side facing volume 1). So the worm passes only through two covers‚Äîfront cover of vol. 1 and back cover of vol. 2‚Äîeach 2 mm thick, totaling 4 mm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5', messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4863476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I need to visualize how books are arranged on a shelf and what the worm actually gnaws through.\n",
       "\n",
       "**Key insight: How books sit on a shelf**\n",
       "\n",
       "When two volumes stand side by side on a bookshelf in reading order (Volume 1, then Volume 2), they look like this from the perspective of someone facing the shelf:\n",
       "\n",
       "- **Volume 1** (on the left): The FIRST page is on the RIGHT side (when the book is closed, you open it and the first page is immediately there), and the LAST page is on the LEFT side (against the back cover)\n",
       "\n",
       "- **Volume 2** (on the right): The FIRST page is on the RIGHT side, and the LAST page is on the LEFT side\n",
       "\n",
       "**What does the worm gnaw through?**\n",
       "\n",
       "The worm goes from:\n",
       "- The first page of Volume 1 (which is on the RIGHT side of Volume 1)\n",
       "- To the last page of Volume 2 (which is on the LEFT side of Volume 2)\n",
       "\n",
       "So the worm's path goes through:\n",
       "1. The back cover of Volume 1 (2 mm)\n",
       "2. All the pages of Volume 2 (2 cm = 20 mm)\n",
       "3. The front cover of Volume 2 (2 mm)\n",
       "\n",
       "Wait, let me reconsider the arrangement more carefully.\n",
       "\n",
       "When books are on a shelf spine-out:\n",
       "- Volume 1 (left book): front cover on left, pages in middle, back cover on right\n",
       "- Volume 2 (right book): front cover on left, pages in middle, back cover on right\n",
       "\n",
       "The first page of Volume 1 is just inside its front cover (left side of Volume 1).\n",
       "The last page of Volume 2 is just inside its back cover (right side of Volume 2).\n",
       "\n",
       "So the worm gnaws from the leftmost part of Volume 1 to the rightmost part of Volume 2, going through:\n",
       "1. Front cover of Volume 1: 2 mm\n",
       "2. All pages of Volume 1: 20 mm\n",
       "3. Back cover of Volume 1: 2 mm\n",
       "4. Front cover of Volume 2: 2 mm\n",
       "5. All pages of Volume 2: 20 mm\n",
       "6. Back cover of Volume 2: 2 mm\n",
       "\n",
       "Total: 2 + 20 + 2 + 2 + 20 + 2 = 48 mm\n",
       "\n",
       "Hmm, but this is the \"trick\" problem. Let me reconsider once more.\n",
       "\n",
       "Actually, the classic version: when Volume 1 and Volume 2 are side by side:\n",
       "- Volume 1's FIRST page is near Volume 2\n",
       "- Volume 2's LAST page is away from Volume 1\n",
       "\n",
       "The worm only goes through:\n",
       "- Back cover of Volume 1: 2 mm\n",
       "- Front cover of Volume 2: 2 mm\n",
       "\n",
       "**Total distance: 4 mm = 0.4 cm**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = anthropic.chat.completions.create(model='claude-sonnet-4-5', messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c57811",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" ‚Äî if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" ‚Äî if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {'role': 'user', 'content': dilemma_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac3c507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Share"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model='gpt-5', messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6804baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I choose **Share**.\n",
       "\n",
       "Here's my reasoning: While \"Steal\" might seem tempting for the chance at $2,000, the rational cooperative strategy is to Share. If I assume my partner is thinking logically about mutual benefit, we both do better by cooperating ($1,000 each) than we do in the mutual defection scenario ($0 each). \n",
       "\n",
       "The risk is that my partner chooses Steal, leaving me with nothing. However, without the ability to communicate or know anything about my partner's tendencies, I'm choosing the strategy that enables the possibility of mutual gain and signals cooperation.\n",
       "\n",
       "**Share** is my final answer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = anthropic.chat.completions.create(model='claude-sonnet-4-5', messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a978f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://localhost:11434/').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11de07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "344fb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(api_key='ollama', base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444a8250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model='llama3.2', messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d161e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Blue is the calm, cool feeling of a gentle breeze on your skin, the peaceful quiet of early morning, and the refreshing sensation of diving into water on a hot day."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model='claude-sonnet-4-5',\n",
    "    messages=[{'role': 'user', 'content': 'Describe the color Blue to someone who has never been able to see in 1 sentence'}],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004a032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
